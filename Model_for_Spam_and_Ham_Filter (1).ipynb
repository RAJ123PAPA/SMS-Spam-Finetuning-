{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Project Discription\n",
        "\n",
        "Model =  DistilBert-base-uncased (for sequence classification)  \n",
        "\n",
        "Downstream task = Classify the SMS into Spam or Ham\n",
        "\n",
        "Dataset = SMSSpamCollection.txt\n",
        "\n",
        "Source = Downloaded the dataset from kaggle"
      ],
      "metadata": {
        "id": "8MwYvaN_Wj5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##Import Dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_C29PMUCTxfo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2luVRI-GUOq"
      },
      "outputs": [],
      "source": [
        "!wget -O SMSSpamCollection.txt https://raw.githubusercontent.com/RAJ123PAPA/SMS-Spam-Finetuning-/main/SMSSpamCollection.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'SMSSpamCollection.txt'\n"
      ],
      "metadata": {
        "id": "bUjDmGMCG4vc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare The Dataset\n",
        "\n",
        "I have txt file of Dataset.So I use pandas to convert it in required format so that the model accepts the file.\n",
        "\n",
        "First we convert into dataframe and the divide it into two list-\n",
        "\n",
        "1). x for message\n",
        "\n",
        "2). y for label"
      ],
      "metadata": {
        "id": "Tkl04YC5UQ9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "import pandas as pd\n",
        "df=messages = pd.read_csv(path, sep='\\t',names=[\"label\", \"message\"])\n",
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "x=list(df['message'])\n",
        "y=list(df['label'])"
      ],
      "metadata": {
        "id": "KtnTKuvPHFPl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In dataset we have two column one is label in which two value either it is Spam or Ham.\n",
        "\n",
        "So convert it into 1 and 0 , so that model can understand .That why I convert spam into 1 and Ham into 0"
      ],
      "metadata": {
        "id": "aA2ZNFv_UjtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=list(pd.get_dummies(y,drop_first=True)['spam'])"
      ],
      "metadata": {
        "id": "KgC8Btn5HIGM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use sklearn module to split our dataset into training and test dataset and we take ratio of spliting 0.2."
      ],
      "metadata": {
        "id": "LsBTj3EGV8ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)"
      ],
      "metadata": {
        "id": "2brxcO5yHcCS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now install Transformers to get pretrained Model.\n",
        "\n",
        "I am using Distilbert model for Sequence classification"
      ],
      "metadata": {
        "id": "aFwg_1Q2WJoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EsFpESECHeUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune The Model\n",
        "\n",
        "This is main code in which I did following task-\n",
        "\n",
        "1.   Load our model and their tokenizer using transformers\n",
        "2. Use cuda (GPU) for fast training the model\n",
        "3. Made a CustomDataset for preproccesing the dataset so we can train the model\n",
        "4.Use Dataloader to divide our dataset into batch and shuffle it ,so that model can train fast and more efficient\n",
        "5.Use AdamW optimizer and CrossEntropyloss loss function\n",
        "6.Start the training loop with number of epoch 3.\n",
        "7.Save the model with name SMSSpam_Filter_model\n"
      ],
      "metadata": {
        "id": "ANnP44ZvXngR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AdamW\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained DistilBERT model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "model.to(device)  # Move the model to GPU\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load your dataset and preprocess it into suitable format\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Define your training and validation data\n",
        "train_texts = X_train  # List of training texts\n",
        "train_labels = y_train # List of corresponding sentiment labels\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Prepare optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch['input_ids'].to(device)  # Move input tensors to GPU\n",
        "        attention_mask = batch['attention_mask'].to(device)  # Move attention mask to GPU\n",
        "        labels = batch['labels'].to(device)  # Move labels to GPU\n",
        "        outputs = model(inputs, attention_mask=attention_mask)[0]\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained(\"SMSSpam_Filter_model\")\n",
        "tokenizer.save_pretrained(\"tokenizer\")\n"
      ],
      "metadata": {
        "id": "3YoHGLQYHr65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "h_52lUHBM3vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path='SMSSpam_Filter_model'"
      ],
      "metadata": {
        "id": "p0331FIFM7W0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_path = '/content/tokenizer/added_tokens.json'\n",
        "destination_path = '/content/SMSSpam_Filter_model'\n",
        "shutil.move(source_path, destination_path)\n",
        "\n",
        "source_path = '/content/tokenizer/special_tokens_map.json'\n",
        "destination_path = '/content/SMSSpam_Filter_model'\n",
        "shutil.move(source_path, destination_path)\n",
        "\n",
        "source_path = '/content/tokenizer/tokenizer_config.json'\n",
        "destination_path = '/content/SMSSpam_Filter_model'\n",
        "shutil.move(source_path, destination_path)\n",
        "\n",
        "\n",
        "source_path = '/content/tokenizer/vocab.txt'\n",
        "destination_path = '/content/SMSSpam_Filter_model'\n",
        "shutil.move(source_path, destination_path)\n"
      ],
      "metadata": {
        "id": "K8rPl0SNQZtK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "569bf8ab-40d6-43c2-d33e-b9c4b1fdc431"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/SMSSpam_Filter_model/vocab.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predicted_label = torch.argmax(outputs.logits).item()\n",
        "    sentiment = \"Spam\" if predicted_label == 1 else \"Ham\"\n",
        "    return sentiment\n",
        "\n",
        "iface = gr.Interface(fn=predict_sentiment, inputs=\"text\", outputs=\"text\")\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "ruXs3liqM9VS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}